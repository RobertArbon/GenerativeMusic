{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RobertArbon/GenerativeMusic/blob/main/fine_tune_riffusion_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWELM8MLRujc"
      },
      "source": [
        "`fine_tune_riffusion.ipynb` was borking the pipeline.  So I need a better way of tuning it.  I will try and do it in a more transparent way here (rather than use the script provided in the examples folder exactly. )\n",
        "\n",
        "I'm basing this training notebook on the script [here](https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image.py)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JEQ7xma9Tc8-",
        "outputId": "5b338e59-014a-41db-a6dc-86e0f7caf882"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/robertarbon/.local/share/virtualenvs/GenerativeMusic-xr8A2ll8/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import argparse\n",
        "import logging\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "import dataclasses\n",
        "\n",
        "\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "\n",
        "import accelerate\n",
        "import datasets\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.checkpoint\n",
        "import transformers\n",
        "from accelerate import Accelerator\n",
        "from accelerate.logging import get_logger\n",
        "from accelerate.state import AcceleratorState\n",
        "from accelerate.utils import ProjectConfiguration, set_seed\n",
        "from datasets import load_dataset\n",
        "# from huggingface_hub import create_repo, upload_folder\n",
        "from packaging import version\n",
        "from torchvision import transforms\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import CLIPTextModel, CLIPTokenizer\n",
        "from transformers.utils import ContextManagers\n",
        "import bitsandbytes as bnb\n",
        "\n",
        "\n",
        "import diffusers\n",
        "from diffusers import AutoencoderKL, DDPMScheduler, StableDiffusionPipeline, UNet2DConditionModel\n",
        "from diffusers.optimization import get_scheduler\n",
        "from diffusers.training_utils import EMAModel, compute_snr\n",
        "from diffusers.utils import check_min_version, deprecate, is_wandb_available, make_image_grid\n",
        "from diffusers.utils.import_utils import is_xformers_available\n",
        "from diffusers.utils.torch_utils import is_compiled_module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "j_u5nkJwVJMq"
      },
      "outputs": [],
      "source": [
        "!mkdir -p smallds\n",
        "!cp ../Data/Dataset/spectrograms/clip_{1..1000..3}.jpg smallds/\n",
        "files = list(Path('smallds').glob('*.jpg'))\n",
        "df = pd.DataFrame(data={'file_name': [x.name for x in files], 'text': ['Guzheng']*len(files)})\n",
        "df.to_csv('smallds/metadata.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "1vHLRxqhXnMG"
      },
      "outputs": [],
      "source": [
        "@dataclasses.dataclass\n",
        "class Config:\n",
        "    output_dir = 'riffusion-guzheng-v2'\n",
        "    logging_dir = 'logs'\n",
        "    gradient_accumulation_steps = 1\n",
        "    mixed_precision = 'bf16'\n",
        "    report_to = 'tensorboard'\n",
        "    seed = 42\n",
        "    pretrained_model_name_or_path = 'riffusion/riffusion-model-v1'\n",
        "    use_xformers = False\n",
        "    use_8bit_adam = False\n",
        "    learning_rate = 1e-4\n",
        "    adam_beta1 = 0.9\n",
        "    adam_beta2 = 0.999\n",
        "    adam_weight_decay = 1e-2\n",
        "    adam_epsilon = 1e-8\n",
        "    max_grad_norm=1.0\n",
        "    train_data_dir='./smallds'\n",
        "    cache_dir=None\n",
        "    image_column = 'file_name'\n",
        "    caption_column = 'text'\n",
        "    train_batch_size = 8\n",
        "    dataloader_num_workers = 8\n",
        "    max_train_steps = None\n",
        "    num_train_epochs = 100\n",
        "    lr_scheduler = \"constant\"\n",
        "    lr_warmup_steps = 500\n",
        "    use_ema = False\n",
        "    tracker_project_name='guzheng'\n",
        "    resume_from_checkpoint = False # put path to checkpoint or 'latest' here.  Looks in output_dir.\n",
        "    noise_offset = 0\n",
        "    input_perturbation = 0.1\n",
        "    prediction_type = None  #Choose between 'epsilon' or 'v_prediction' or leave `None`. If left to `None` the default prediction type of the scheduler: `noise_scheduler.config.prediction_type` is chosen.\n",
        "    snr_gamme = None\n",
        "    checkpointing_steps = 50\n",
        "    checkpoints_total_limit = 5\n",
        "    validation_prompts = 'Solo Guzhen Music'\n",
        "    validation_epochs = 1\n",
        "\n",
        "args = Config()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "lXErorh6UdMv"
      },
      "outputs": [],
      "source": [
        "def training_loop(args):\n",
        "\n",
        "    logging_dir = os.path.join(args.output_dir, args.logging_dir)\n",
        "    accelerator_project_config = ProjectConfiguration(project_dir=args.output_dir, logging_dir=logging_dir)\n",
        "\n",
        "\n",
        "    accelerator = Accelerator(\n",
        "      gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
        "      mixed_precision=args.mixed_precision,\n",
        "      log_with=args.report_to,\n",
        "      project_config=accelerator_project_config,\n",
        "    )\n",
        "\n",
        "    set_seed(args.seed)\n",
        "\n",
        "    if accelerator.is_main_process:\n",
        "        os.makedirs(args.output_dir, exist_ok=True)\n",
        "\n",
        "    noise_scheduler = DDPMScheduler.from_pretrained(args.pretrained_model_name_or_path,\n",
        "                                                  subfolder=\"scheduler\")\n",
        "    tokenizer = CLIPTokenizer.from_pretrained(args.pretrained_model_name_or_path,\n",
        "                                            subfolder=\"tokenizer\")\n",
        "\n",
        "    text_encoder = CLIPTextModel.from_pretrained(\n",
        "            args.pretrained_model_name_or_path, subfolder=\"text_encoder\")\n",
        "\n",
        "    vae = AutoencoderKL.from_pretrained(\n",
        "            args.pretrained_model_name_or_path, subfolder=\"vae\")\n",
        "\n",
        "    unet = UNet2DConditionModel.from_pretrained(\n",
        "        args.pretrained_model_name_or_path, subfolder=\"unet\")\n",
        "\n",
        "    text_encoder.requires_grad_(False)\n",
        "    vae.requires_grad_(False)\n",
        "    unet.train()\n",
        "    if args.use_xformers:\n",
        "        unet.enable_xformers_memory_efficient_attention()\n",
        "\n",
        "\n",
        "    def save_model_hook(models, weights, output_dir):\n",
        "        if accelerator.is_main_process:\n",
        "            for i, model in enumerate(models):\n",
        "                model.save_pretrained(os.path.join(output_dir, \"unet\"))\n",
        "                weights.pop()\n",
        "\n",
        "    def load_model_hook(models, input_dir):\n",
        "        for i in range(len(models)):\n",
        "            # pop models so that they are not loaded again\n",
        "            model = models.pop()\n",
        "\n",
        "            # load diffusers style into model\n",
        "            load_model = UNet2DConditionModel.from_pretrained(input_dir, subfolder=\"unet\")\n",
        "            model.register_to_config(**load_model.config)\n",
        "\n",
        "            model.load_state_dict(load_model.state_dict())\n",
        "            del load_model\n",
        "\n",
        "    accelerator.register_save_state_pre_hook(save_model_hook)\n",
        "    accelerator.register_load_state_pre_hook(load_model_hook)\n",
        "\n",
        "    if args.gradient_checkpointing:\n",
        "      unet.enable_gradient_checkpointing()\n",
        "\n",
        "\n",
        "    # if args.scale_lr:\n",
        "    #     args.learning_rate = (\n",
        "    #         args.learning_rate * args.gradient_accumulation_steps * args.train_batch_size\n",
        "    #     )\n",
        "\n",
        "    # Initialize the optimizer\n",
        "    if args.use_8bit_adam:\n",
        "        optimizer_cls = bnb.optim.AdamW8bit\n",
        "    else:\n",
        "        optimizer_cls = torch.optim.AdamW\n",
        "\n",
        "    optimizer = optimizer_cls(\n",
        "        unet.parameters(),\n",
        "        lr=args.learning_rate,\n",
        "        betas=(args.adam_beta1, args.adam_beta2),\n",
        "        weight_decay=args.adam_weight_decay,\n",
        "        eps=args.adam_epsilon,\n",
        "    )\n",
        "\n",
        "    # Get the datasets: you can either provide your own training and evaluation files (see below)\n",
        "    # or specify a Dataset from the hub (the dataset will be downloaded automatically from the datasets Hub).\n",
        "\n",
        "    # In distributed training, the load_dataset function guarantees that only one local process can concurrently\n",
        "    # download the dataset.\n",
        "\n",
        "    data_files = {}\n",
        "\n",
        "    data_files[\"train\"] = os.path.join(args.train_data_dir, \"**\")\n",
        "    dataset = load_dataset(\n",
        "        \"imagefolder\",\n",
        "        data_files=data_files,\n",
        "        cache_dir=args.cache_dir,\n",
        "    )\n",
        "        # See more about loading custom images at\n",
        "        # https://huggingface.co/docs/datasets/v2.4.0/en/image_load#imagefolder\n",
        "\n",
        "    # Preprocessing the datasets.\n",
        "    # We need to tokenize inputs and targets.\n",
        "    column_names = dataset[\"train\"].column_names\n",
        "\n",
        "    # 6. Get the column names for input/target.\n",
        "    dataset_columns = DATASET_NAME_MAPPING.get(args.dataset_name, None)\n",
        "\n",
        "    image_column = args.image_column\n",
        "    if image_column not in column_names:\n",
        "        raise ValueError(\n",
        "            f\"--image_column' value '{args.image_column}' needs to be one of: {', '.join(column_names)}\"\n",
        "            )\n",
        "\n",
        "    caption_column = args.caption_column\n",
        "    if caption_column not in column_names:\n",
        "        raise ValueError(\n",
        "            f\"--caption_column' value '{args.caption_column}' needs to be one of: {', '.join(column_names)}\"\n",
        "        )\n",
        "\n",
        "    # Preprocessing the datasets.\n",
        "    # We need to tokenize input captions and transform the images.\n",
        "    def tokenize_captions(examples, is_train=True):\n",
        "        captions = []\n",
        "        for caption in examples[caption_column]:\n",
        "            if isinstance(caption, str):\n",
        "                captions.append(caption)\n",
        "            elif isinstance(caption, (list, np.ndarray)):\n",
        "                # take a random caption if there are multiple\n",
        "                captions.append(random.choice(caption) if is_train else caption[0])\n",
        "            else:\n",
        "                raise ValueError(\n",
        "                    f\"Caption column `{caption_column}` should contain either strings or lists of strings.\"\n",
        "                )\n",
        "        inputs = tokenizer(\n",
        "            captions, max_length=tokenizer.model_max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
        "        )\n",
        "        return inputs.input_ids\n",
        "\n",
        "    # Preprocessing the datasets.\n",
        "    train_transforms = transforms.Compose(\n",
        "        [\n",
        "            transforms.Resize(args.resolution, interpolation=transforms.InterpolationMode.BILINEAR),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.5], [0.5]),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    def preprocess_train(examples):\n",
        "        images = [image.convert(\"RGB\") for image in examples[image_column]]\n",
        "        examples[\"pixel_values\"] = [train_transforms(image) for image in images]\n",
        "        examples[\"input_ids\"] = tokenize_captions(examples)\n",
        "        return examples\n",
        "\n",
        "    with accelerator.main_process_first():\n",
        "        if args.max_train_samples is not None:\n",
        "            dataset[\"train\"] = dataset[\"train\"].shuffle(seed=args.seed).select(range(args.max_train_samples))\n",
        "        # Set the training transforms\n",
        "        train_dataset = dataset[\"train\"].with_transform(preprocess_train)\n",
        "\n",
        "    def collate_fn(examples):\n",
        "        pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
        "        pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n",
        "        input_ids = torch.stack([example[\"input_ids\"] for example in examples])\n",
        "        return {\"pixel_values\": pixel_values, \"input_ids\": input_ids}\n",
        "\n",
        "    # DataLoaders creation:\n",
        "    train_dataloader = torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        shuffle=True,\n",
        "        collate_fn=collate_fn,\n",
        "        batch_size=args.train_batch_size,\n",
        "        num_workers=args.dataloader_num_workers,\n",
        "    )\n",
        "\n",
        "    # Scheduler and math around the number of training steps.\n",
        "    overrode_max_train_steps = False\n",
        "    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n",
        "    if args.max_train_steps is None:\n",
        "        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n",
        "        overrode_max_train_steps = True\n",
        "\n",
        "    lr_scheduler = get_scheduler(\n",
        "        args.lr_scheduler,\n",
        "        optimizer=optimizer,\n",
        "        num_warmup_steps=args.lr_warmup_steps * accelerator.num_processes,\n",
        "        num_training_steps=args.max_train_steps * accelerator.num_processes,\n",
        "    )\n",
        "\n",
        "    # Prepare everything with our `accelerator`.\n",
        "    unet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
        "        unet, optimizer, train_dataloader, lr_scheduler\n",
        "    )\n",
        "\n",
        "    if args.use_ema:\n",
        "        ema_unet.to(accelerator.device)\n",
        "\n",
        "    # For mixed precision training we cast all non-trainable weights (vae, non-lora text_encoder and non-lora unet) to half-precision\n",
        "    # as these weights are only used for inference, keeping weights in full precision is not required.\n",
        "    weight_dtype = torch.float32\n",
        "    if accelerator.mixed_precision == \"fp16\":\n",
        "        weight_dtype = torch.float16\n",
        "        args.mixed_precision = accelerator.mixed_precision\n",
        "    elif accelerator.mixed_precision == \"bf16\":\n",
        "        weight_dtype = torch.bfloat16\n",
        "        args.mixed_precision = accelerator.mixed_precision\n",
        "\n",
        "    # Move text_encode and vae to gpu and cast to weight_dtype\n",
        "    text_encoder.to(accelerator.device, dtype=weight_dtype)\n",
        "    vae.to(accelerator.device, dtype=weight_dtype)\n",
        "\n",
        "    # We need to recalculate our total training steps as the size of the training dataloader may have changed.\n",
        "    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n",
        "    if overrode_max_train_steps:\n",
        "        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n",
        "    # Afterwards we recalculate our number of training epochs\n",
        "    args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n",
        "\n",
        "    # We need to initialize the trackers we use, and also store our configuration.\n",
        "    # The trackers initializes automatically on the main process.\n",
        "    if accelerator.is_main_process:\n",
        "        tracker_config = dict(vars(args))\n",
        "        tracker_config.pop(\"validation_prompts\")\n",
        "        accelerator.init_trackers(args.tracker_project_name, tracker_config)\n",
        "\n",
        "    # Function for unwrapping if model was compiled with `torch.compile`.\n",
        "    def unwrap_model(model):\n",
        "        model = accelerator.unwrap_model(model)\n",
        "        model = model._orig_mod if is_compiled_module(model) else model\n",
        "        return model\n",
        "\n",
        "    # Train!\n",
        "    total_batch_size = args.train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n",
        "\n",
        "    logger.info(\"***** Running training *****\")\n",
        "    logger.info(f\"  Num examples = {len(train_dataset)}\")\n",
        "    logger.info(f\"  Num Epochs = {args.num_train_epochs}\")\n",
        "    logger.info(f\"  Instantaneous batch size per device = {args.train_batch_size}\")\n",
        "    logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n",
        "    logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n",
        "    logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\n",
        "    global_step = 0\n",
        "    first_epoch = 0\n",
        "\n",
        "    # Potentially load in the weights and states from a previous save\n",
        "    if args.resume_from_checkpoint:\n",
        "        if args.resume_from_checkpoint != \"latest\":\n",
        "            path = os.path.basename(args.resume_from_checkpoint)\n",
        "        else:\n",
        "            # Get the most recent checkpoint\n",
        "            dirs = os.listdir(args.output_dir)\n",
        "            dirs = [d for d in dirs if d.startswith(\"checkpoint\")]\n",
        "            dirs = sorted(dirs, key=lambda x: int(x.split(\"-\")[1]))\n",
        "            path = dirs[-1] if len(dirs) > 0 else None\n",
        "\n",
        "        if path is None:\n",
        "            accelerator.print(\n",
        "                f\"Checkpoint '{args.resume_from_checkpoint}' does not exist. Starting a new training run.\"\n",
        "            )\n",
        "            args.resume_from_checkpoint = None\n",
        "            initial_global_step = 0\n",
        "        else:\n",
        "            accelerator.print(f\"Resuming from checkpoint {path}\")\n",
        "            accelerator.load_state(os.path.join(args.output_dir, path))\n",
        "            global_step = int(path.split(\"-\")[1])\n",
        "\n",
        "            initial_global_step = global_step\n",
        "            first_epoch = global_step // num_update_steps_per_epoch\n",
        "\n",
        "    else:\n",
        "        initial_global_step = 0\n",
        "\n",
        "    progress_bar = tqdm(\n",
        "        range(0, args.max_train_steps),\n",
        "        initial=initial_global_step,\n",
        "        desc=\"Steps\",\n",
        "        # Only show the progress bar once on each machine.\n",
        "        disable=not accelerator.is_local_main_process,\n",
        "    )\n",
        "\n",
        "    for epoch in range(first_epoch, args.num_train_epochs):\n",
        "        train_loss = 0.0\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            with accelerator.accumulate(unet):\n",
        "                # Convert images to latent space\n",
        "                latents = vae.encode(batch[\"pixel_values\"].to(weight_dtype)).latent_dist.sample()\n",
        "                latents = latents * vae.config.scaling_factor\n",
        "\n",
        "                # Sample noise that we'll add to the latents\n",
        "                noise = torch.randn_like(latents)\n",
        "                if args.noise_offset:\n",
        "                    # https://www.crosslabs.org//blog/diffusion-with-offset-noise\n",
        "                    noise += args.noise_offset * torch.randn(\n",
        "                        (latents.shape[0], latents.shape[1], 1, 1), device=latents.device\n",
        "                    )\n",
        "                if args.input_perturbation:\n",
        "                    new_noise = noise + args.input_perturbation * torch.randn_like(noise)\n",
        "                bsz = latents.shape[0]\n",
        "                # Sample a random timestep for each image\n",
        "                timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device)\n",
        "                timesteps = timesteps.long()\n",
        "\n",
        "                # Add noise to the latents according to the noise magnitude at each timestep\n",
        "                # (this is the forward diffusion process)\n",
        "                if args.input_perturbation:\n",
        "                    noisy_latents = noise_scheduler.add_noise(latents, new_noise, timesteps)\n",
        "                else:\n",
        "                    noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
        "\n",
        "                # Get the text embedding for conditioning\n",
        "                encoder_hidden_states = text_encoder(batch[\"input_ids\"], return_dict=False)[0]\n",
        "\n",
        "                # Get the target for loss depending on the prediction type\n",
        "                if args.prediction_type is not None:\n",
        "                    # set prediction_type of scheduler if defined\n",
        "                    noise_scheduler.register_to_config(prediction_type=args.prediction_type)\n",
        "\n",
        "                if noise_scheduler.config.prediction_type == \"epsilon\":\n",
        "                    target = noise\n",
        "                elif noise_scheduler.config.prediction_type == \"v_prediction\":\n",
        "                    target = noise_scheduler.get_velocity(latents, noise, timesteps)\n",
        "                else:\n",
        "                    raise ValueError(f\"Unknown prediction type {noise_scheduler.config.prediction_type}\")\n",
        "\n",
        "                # Predict the noise residual and compute loss\n",
        "                model_pred = unet(noisy_latents, timesteps, encoder_hidden_states, return_dict=False)[0]\n",
        "\n",
        "                if args.snr_gamma is None:\n",
        "                    loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\n",
        "                else:\n",
        "                    # Compute loss-weights as per Section 3.4 of https://arxiv.org/abs/2303.09556.\n",
        "                    # Since we predict the noise instead of x_0, the original formulation is slightly changed.\n",
        "                    # This is discussed in Section 4.2 of the same paper.\n",
        "                    snr = compute_snr(noise_scheduler, timesteps)\n",
        "                    mse_loss_weights = torch.stack([snr, args.snr_gamma * torch.ones_like(timesteps)], dim=1).min(\n",
        "                        dim=1\n",
        "                    )[0]\n",
        "                    if noise_scheduler.config.prediction_type == \"epsilon\":\n",
        "                        mse_loss_weights = mse_loss_weights / snr\n",
        "                    elif noise_scheduler.config.prediction_type == \"v_prediction\":\n",
        "                        mse_loss_weights = mse_loss_weights / (snr + 1)\n",
        "\n",
        "                    loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"none\")\n",
        "                    loss = loss.mean(dim=list(range(1, len(loss.shape)))) * mse_loss_weights\n",
        "                    loss = loss.mean()\n",
        "\n",
        "                # Gather the losses across all processes for logging (if we use distributed training).\n",
        "                avg_loss = accelerator.gather(loss.repeat(args.train_batch_size)).mean()\n",
        "                train_loss += avg_loss.item() / args.gradient_accumulation_steps\n",
        "\n",
        "                # Backpropagate\n",
        "                accelerator.backward(loss)\n",
        "                if accelerator.sync_gradients:\n",
        "                    accelerator.clip_grad_norm_(unet.parameters(), args.max_grad_norm)\n",
        "                optimizer.step()\n",
        "                lr_scheduler.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            # Checks if the accelerator has performed an optimization step behind the scenes\n",
        "            if accelerator.sync_gradients:\n",
        "                if args.use_ema:\n",
        "                    ema_unet.step(unet.parameters())\n",
        "                progress_bar.update(1)\n",
        "                global_step += 1\n",
        "                accelerator.log({\"train_loss\": train_loss}, step=global_step)\n",
        "                train_loss = 0.0\n",
        "\n",
        "                if global_step % args.checkpointing_steps == 0:\n",
        "                    if accelerator.is_main_process:\n",
        "                        # _before_ saving state, check if this save would set us over the `checkpoints_total_limit`\n",
        "                        if args.checkpoints_total_limit is not None:\n",
        "                            checkpoints = os.listdir(args.output_dir)\n",
        "                            checkpoints = [d for d in checkpoints if d.startswith(\"checkpoint\")]\n",
        "                            checkpoints = sorted(checkpoints, key=lambda x: int(x.split(\"-\")[1]))\n",
        "\n",
        "                            # before we save the new checkpoint, we need to have at _most_ `checkpoints_total_limit - 1` checkpoints\n",
        "                            if len(checkpoints) >= args.checkpoints_total_limit:\n",
        "                                num_to_remove = len(checkpoints) - args.checkpoints_total_limit + 1\n",
        "                                removing_checkpoints = checkpoints[0:num_to_remove]\n",
        "\n",
        "                                logger.info(\n",
        "                                    f\"{len(checkpoints)} checkpoints already exist, removing {len(removing_checkpoints)} checkpoints\"\n",
        "                                )\n",
        "                                logger.info(f\"removing checkpoints: {', '.join(removing_checkpoints)}\")\n",
        "\n",
        "                                for removing_checkpoint in removing_checkpoints:\n",
        "                                    removing_checkpoint = os.path.join(args.output_dir, removing_checkpoint)\n",
        "                                    shutil.rmtree(removing_checkpoint)\n",
        "\n",
        "                        save_path = os.path.join(args.output_dir, f\"checkpoint-{global_step}\")\n",
        "                        accelerator.save_state(save_path)\n",
        "                        logger.info(f\"Saved state to {save_path}\")\n",
        "\n",
        "            logs = {\"step_loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0]}\n",
        "            progress_bar.set_postfix(**logs)\n",
        "\n",
        "            if global_step >= args.max_train_steps:\n",
        "                break\n",
        "\n",
        "        if accelerator.is_main_process:\n",
        "            if args.validation_prompts is not None and epoch % args.validation_epochs == 0:\n",
        "                if args.use_ema:\n",
        "                    # Store the UNet parameters temporarily and load the EMA parameters to perform inference.\n",
        "                    ema_unet.store(unet.parameters())\n",
        "                    ema_unet.copy_to(unet.parameters())\n",
        "                log_validation(\n",
        "                    vae,\n",
        "                    text_encoder,\n",
        "                    tokenizer,\n",
        "                    unet,\n",
        "                    args,\n",
        "                    accelerator,\n",
        "                    weight_dtype,\n",
        "                    global_step,\n",
        "                )\n",
        "                if args.use_ema:\n",
        "                    # Switch back to the original UNet parameters.\n",
        "                    ema_unet.restore(unet.parameters())\n",
        "\n",
        "    # Create the pipeline using the trained modules and save it.\n",
        "    accelerator.wait_for_everyone()\n",
        "    if accelerator.is_main_process:\n",
        "        unet = unwrap_model(unet)\n",
        "        if args.use_ema:\n",
        "            ema_unet.copy_to(unet.parameters())\n",
        "\n",
        "        pipeline = StableDiffusionPipeline.from_pretrained(\n",
        "            args.pretrained_model_name_or_path,\n",
        "            text_encoder=text_encoder,\n",
        "            vae=vae,\n",
        "            unet=unet,\n",
        "            revision=args.revision,\n",
        "            variant=args.variant,\n",
        "        )\n",
        "        pipeline.save_pretrained(args.output_dir)\n",
        "\n",
        "        # Run a final round of inference.\n",
        "        images = []\n",
        "        if args.validation_prompts is not None:\n",
        "            logger.info(\"Running inference for collecting generated images...\")\n",
        "            pipeline = pipeline.to(accelerator.device)\n",
        "            pipeline.torch_dtype = weight_dtype\n",
        "            pipeline.set_progress_bar_config(disable=True)\n",
        "\n",
        "            if args.enable_xformers_memory_efficient_attention:\n",
        "                pipeline.enable_xformers_memory_efficient_attention()\n",
        "\n",
        "            if args.seed is None:\n",
        "                generator = None\n",
        "            else:\n",
        "                generator = torch.Generator(device=accelerator.device).manual_seed(args.seed)\n",
        "\n",
        "            for i in range(len(args.validation_prompts)):\n",
        "                with torch.autocast(\"cuda\"):\n",
        "                    image = pipeline(args.validation_prompts[i], num_inference_steps=20, generator=generator).images[0]\n",
        "                    imgage.save(f\"{args.output_dir}/final_images_prompt_{i}.jpg\")\n",
        "\n",
        "\n",
        "    accelerator.end_training()\n",
        "\n",
        "# training_loop(args)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "xmrfjQbZ4FxN",
        "outputId": "f8c3ec69-39d5-4313-b786-a740399559f0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/robertarbon/.local/share/virtualenvs/GenerativeMusic-xr8A2ll8/lib/python3.10/site-packages/accelerate/accelerator.py:387: UserWarning: `log_with=tensorboard` was passed but no supported trackers are currently installed.\n",
            "  warnings.warn(f\"`log_with={log_with}` was passed but no supported trackers are currently installed.\")\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'Config' object has no attribute 'gradient_checkpointing'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtraining_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[6], line 61\u001b[0m, in \u001b[0;36mtraining_loop\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     58\u001b[0m accelerator\u001b[38;5;241m.\u001b[39mregister_save_state_pre_hook(save_model_hook)\n\u001b[1;32m     59\u001b[0m accelerator\u001b[38;5;241m.\u001b[39mregister_load_state_pre_hook(load_model_hook)\n\u001b[0;32m---> 61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient_checkpointing\u001b[49m:\n\u001b[1;32m     62\u001b[0m   unet\u001b[38;5;241m.\u001b[39menable_gradient_checkpointing()\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# if args.scale_lr:\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m#     args.learning_rate = (\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m#         args.learning_rate * args.gradient_accumulation_steps * args.train_batch_size\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m#     )\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Initialize the optimizer\u001b[39;00m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Config' object has no attribute 'gradient_checkpointing'"
          ]
        }
      ],
      "source": [
        "training_loop(args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58l2wtOR_3BS"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
