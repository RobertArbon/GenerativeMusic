{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Argument #4: Padding size should be less than the corresponding input dimension, but got: padding (8820, 8820) at dimension 2 of input [1, 512, 882]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 181\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;66;03m# Reconstruct the waveform\u001b[39;00m\n\u001b[1;32m    180\u001b[0m amplitudes_linear \u001b[38;5;241m=\u001b[39m inverse_mel_scaler(amplitudes_mel)\n\u001b[0;32m--> 181\u001b[0m waveform \u001b[38;5;241m=\u001b[39m \u001b[43minverse_spectrogram_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamplitudes_linear\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;66;03m# # Convert to audio segment\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;66;03m# segment = audio_from_waveform(\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m#     samples=waveform.cpu().numpy(),\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m#     normalize=True,\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/GenerativeMusic-xr8A2ll8/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/GenerativeMusic-xr8A2ll8/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/GenerativeMusic-xr8A2ll8/lib/python3.10/site-packages/torchaudio/transforms/_transforms.py:285\u001b[0m, in \u001b[0;36mGriffinLim.forward\u001b[0;34m(self, specgram)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, specgram: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    276\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;124;03m        specgram (Tensor):\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;124;03m        Tensor: waveform of (..., time), where time equals the ``length`` parameter if given.\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgriffinlim\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m        \u001b[49m\u001b[43mspecgram\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_fft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhop_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwin_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpower\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlength\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrand_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/GenerativeMusic-xr8A2ll8/lib/python3.10/site-packages/torchaudio/functional/functional.py:326\u001b[0m, in \u001b[0;36mgriffinlim\u001b[0;34m(specgram, window, n_fft, hop_length, win_length, power, n_iter, momentum, length, rand_init)\u001b[0m\n\u001b[1;32m    321\u001b[0m inverse \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mistft(\n\u001b[1;32m    322\u001b[0m     specgram \u001b[38;5;241m*\u001b[39m angles, n_fft\u001b[38;5;241m=\u001b[39mn_fft, hop_length\u001b[38;5;241m=\u001b[39mhop_length, win_length\u001b[38;5;241m=\u001b[39mwin_length, window\u001b[38;5;241m=\u001b[39mwindow, length\u001b[38;5;241m=\u001b[39mlength\n\u001b[1;32m    323\u001b[0m )\n\u001b[1;32m    325\u001b[0m \u001b[38;5;66;03m# Rebuild the spectrogram\u001b[39;00m\n\u001b[0;32m--> 326\u001b[0m rebuilt \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstft\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minverse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_fft\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_fft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhop_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhop_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwin_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwin_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwindow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcenter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreflect\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnormalized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43monesided\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;66;03m# Update our phase estimates\u001b[39;00m\n\u001b[1;32m    340\u001b[0m angles \u001b[38;5;241m=\u001b[39m rebuilt\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/GenerativeMusic-xr8A2ll8/lib/python3.10/site-packages/torch/functional.py:658\u001b[0m, in \u001b[0;36mstft\u001b[0;34m(input, n_fft, hop_length, win_length, window, center, pad_mode, normalized, onesided, return_complex)\u001b[0m\n\u001b[1;32m    656\u001b[0m     extended_shape \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m3\u001b[39m \u001b[38;5;241m-\u001b[39m signal_dim) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m    657\u001b[0m     pad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(n_fft \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 658\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mextended_shape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mpad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    659\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39msignal_dim:])\n\u001b[1;32m    660\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mstft(\u001b[38;5;28minput\u001b[39m, n_fft, hop_length, win_length, window,  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    661\u001b[0m                 normalized, onesided, return_complex)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/GenerativeMusic-xr8A2ll8/lib/python3.10/site-packages/torch/nn/functional.py:4495\u001b[0m, in \u001b[0;36mpad\u001b[0;34m(input, pad, mode, value)\u001b[0m\n\u001b[1;32m   4488\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pad) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreplicate\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m   4489\u001b[0m             \u001b[38;5;66;03m# Use slow decomp whose backward will be in terms of index_put.\u001b[39;00m\n\u001b[1;32m   4490\u001b[0m             \u001b[38;5;66;03m# importlib is required because the import cannot be top level\u001b[39;00m\n\u001b[1;32m   4491\u001b[0m             \u001b[38;5;66;03m# (cycle) and cannot be nested (TS doesn't support)\u001b[39;00m\n\u001b[1;32m   4492\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch._decomp.decompositions\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mreplication_pad2d(\n\u001b[1;32m   4493\u001b[0m                 \u001b[38;5;28minput\u001b[39m, pad\n\u001b[1;32m   4494\u001b[0m             )\n\u001b[0;32m-> 4495\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Argument #4: Padding size should be less than the corresponding input dimension, but got: padding (8820, 8820) at dimension 2 of input [1, 512, 882]"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import typing as T\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "import io\n",
    "import numpy as np\n",
    "import pydub\n",
    "from scipy.io import wavfile\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "def audio_from_waveform(\n",
    "    samples: np.ndarray, sample_rate: int, normalize: bool = False\n",
    ") -> pydub.AudioSegment:\n",
    "    \"\"\"\n",
    "    Convert a numpy array of samples of a waveform to an audio segment.\n",
    "\n",
    "    Args:\n",
    "        samples: (channels, samples) array\n",
    "    \"\"\"\n",
    "    # Normalize volume to fit in int16\n",
    "    if normalize:\n",
    "        samples *= np.iinfo(np.int16).max / np.max(np.abs(samples))\n",
    "\n",
    "    # Transpose and convert to int16\n",
    "    samples = samples.transpose(1, 0)\n",
    "    samples = samples.astype(np.int16)\n",
    "\n",
    "    # Write to the bytes of a WAV file\n",
    "    wav_bytes = io.BytesIO()\n",
    "    wavfile.write(wav_bytes, sample_rate, samples)\n",
    "    wav_bytes.seek(0)\n",
    "\n",
    "    # Read into pydub\n",
    "    return pydub.AudioSegment.from_wav(wav_bytes)\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class SpectrogramParams:\n",
    "    \"\"\"\n",
    "    Parameters for the conversion from audio to spectrograms to images and back.\n",
    "\n",
    "    Includes helpers to convert to and from EXIF tags, allowing these parameters to be stored\n",
    "    within spectrogram images.\n",
    "\n",
    "    To understand what these parameters do and to customize them, read `spectrogram_converter.py`\n",
    "    and the linked torchaudio documentation.\n",
    "    \"\"\"\n",
    "\n",
    "    # Whether the audio is stereo or mono\n",
    "    stereo: bool = False\n",
    "\n",
    "    # FFT parameters\n",
    "    sample_rate: int = 44100\n",
    "    step_size_ms: int = 10\n",
    "    window_duration_ms: int = 100\n",
    "    padded_duration_ms: int = 400\n",
    "\n",
    "    # Mel scale parameters\n",
    "    num_frequencies: int = 512\n",
    "    # TODO(hayk): Set these to [20, 20000] for newer models\n",
    "    min_frequency: int = 0\n",
    "    max_frequency: int = 10000\n",
    "    mel_scale_norm: T.Optional[str] = None\n",
    "    mel_scale_type: str = \"htk\"\n",
    "    max_mel_iters: int = 200\n",
    "\n",
    "    # Griffin Lim parameters\n",
    "    num_griffin_lim_iters: int = 32\n",
    "\n",
    "    # Image parameterization\n",
    "    power_for_image: float = 0.25\n",
    "\n",
    "    class ExifTags(Enum):\n",
    "        \"\"\"\n",
    "        Custom EXIF tags for the spectrogram image.\n",
    "        \"\"\"\n",
    "\n",
    "        SAMPLE_RATE = 11000\n",
    "        STEREO = 11005\n",
    "        STEP_SIZE_MS = 11010\n",
    "        WINDOW_DURATION_MS = 11020\n",
    "        PADDED_DURATION_MS = 11030\n",
    "\n",
    "        NUM_FREQUENCIES = 11040\n",
    "        MIN_FREQUENCY = 11050\n",
    "        MAX_FREQUENCY = 11060\n",
    "\n",
    "        POWER_FOR_IMAGE = 11070\n",
    "        MAX_VALUE = 11080\n",
    "\n",
    "    @property\n",
    "    def n_fft(self) -> int:\n",
    "        \"\"\"\n",
    "        The number of samples in each STFT window, with padding.\n",
    "        \"\"\"\n",
    "        return int(self.padded_duration_ms / 1000.0 * self.sample_rate)\n",
    "\n",
    "    @property\n",
    "    def win_length(self) -> int:\n",
    "        \"\"\"\n",
    "        The number of samples in each STFT window.\n",
    "        \"\"\"\n",
    "        return int(self.window_duration_ms / 1000.0 * self.sample_rate)\n",
    "\n",
    "    @property\n",
    "    def hop_length(self) -> int:\n",
    "        \"\"\"\n",
    "        The number of samples between each STFT window.\n",
    "        \"\"\"\n",
    "        return int(self.step_size_ms / 1000.0 * self.sample_rate)\n",
    "\n",
    "    def to_exif(self) -> T.Dict[int, T.Any]:\n",
    "        \"\"\"\n",
    "        Return a dictionary of EXIF tags for the current values.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            self.ExifTags.SAMPLE_RATE.value: self.sample_rate,\n",
    "            self.ExifTags.STEREO.value: self.stereo,\n",
    "            self.ExifTags.STEP_SIZE_MS.value: self.step_size_ms,\n",
    "            self.ExifTags.WINDOW_DURATION_MS.value: self.window_duration_ms,\n",
    "            self.ExifTags.PADDED_DURATION_MS.value: self.padded_duration_ms,\n",
    "            self.ExifTags.NUM_FREQUENCIES.value: self.num_frequencies,\n",
    "            self.ExifTags.MIN_FREQUENCY.value: self.min_frequency,\n",
    "            self.ExifTags.MAX_FREQUENCY.value: self.max_frequency,\n",
    "            self.ExifTags.POWER_FOR_IMAGE.value: float(self.power_for_image),\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def from_exif(cls, exif: T.Mapping[int, T.Any]): \n",
    "        \"\"\"\n",
    "        Create a SpectrogramParams object from the EXIF tags of the given image.\n",
    "        \"\"\"\n",
    "        # TODO(hayk): Handle missing tags\n",
    "        return cls(\n",
    "            sample_rate=exif[cls.ExifTags.SAMPLE_RATE.value],\n",
    "            stereo=bool(exif[cls.ExifTags.STEREO.value]),\n",
    "            step_size_ms=exif[cls.ExifTags.STEP_SIZE_MS.value],\n",
    "            window_duration_ms=exif[cls.ExifTags.WINDOW_DURATION_MS.value],\n",
    "            padded_duration_ms=exif[cls.ExifTags.PADDED_DURATION_MS.value],\n",
    "            num_frequencies=exif[cls.ExifTags.NUM_FREQUENCIES.value],\n",
    "            min_frequency=exif[cls.ExifTags.MIN_FREQUENCY.value],\n",
    "            max_frequency=exif[cls.ExifTags.MAX_FREQUENCY.value],\n",
    "            power_for_image=exif[cls.ExifTags.POWER_FOR_IMAGE.value],\n",
    "        )\n",
    "\n",
    "\n",
    "params = SpectrogramParams()\n",
    "\n",
    "inverse_spectrogram_func = torchaudio.transforms.GriffinLim(\n",
    "            n_fft=params.n_fft,\n",
    "            n_iter=params.num_griffin_lim_iters,\n",
    "            win_length=params.win_length,\n",
    "            hop_length=params.hop_length,\n",
    "            window_fn=torch.hann_window,\n",
    "            power=1.0,\n",
    "            wkwargs=None,\n",
    "            momentum=0.99,\n",
    "            length=None,\n",
    "            rand_init=True,\n",
    "        ).to(device)\n",
    "\n",
    "\n",
    "inverse_mel_scaler = torchaudio.transforms.InverseMelScale(\n",
    "            n_stft=params.n_fft // 2 + 1,\n",
    "            n_mels=params.num_frequencies,\n",
    "            sample_rate=params.sample_rate,\n",
    "            f_min=params.min_frequency,\n",
    "            f_max=params.max_frequency,\n",
    "            norm=params.mel_scale_norm,\n",
    "            mel_scale=params.mel_scale_type,\n",
    "        ).to(device)\n",
    "\n",
    "\n",
    "spectrogram = np.array(Image.open('riffusion-guzheng-v2/final_images_prompt_2.jpg')).astype(np.float32)\n",
    "amplitudes_mel = torch.from_numpy(spectrogram).to(device)\n",
    "# Reconstruct the waveform\n",
    "amplitudes_linear = inverse_mel_scaler(amplitudes_mel)\n",
    "waveform = inverse_spectrogram_func(amplitudes_linear)\n",
    "\n",
    "# # Convert to audio segment\n",
    "# segment = audio_from_waveform(\n",
    "#     samples=waveform.cpu().numpy(),\n",
    "#     sample_rate=params.sample_rate,\n",
    "#     # Normalize the waveform to the range [-1, 1]\n",
    "#     normalize=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GenerativeMusic-xr8A2ll8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
