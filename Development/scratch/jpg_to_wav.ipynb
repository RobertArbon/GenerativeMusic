{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import typing as T\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "import io\n",
    "import numpy as np\n",
    "import pydub\n",
    "from scipy.io import wavfile\n",
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "\n",
    "\n",
    "def spectrogram_from_image(\n",
    "    image: Image.Image,\n",
    "    power: float = 0.25,\n",
    "    stereo: bool = False,\n",
    "    max_value: float = 30e6,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute a spectrogram magnitude array from a spectrogram image.\n",
    "\n",
    "    This is the inverse of image_from_spectrogram, except for discretization error from\n",
    "    quantizing to uint8.\n",
    "\n",
    "    Args:\n",
    "        image: (frequency, time, channels)\n",
    "        power: The power curve applied to the spectrogram\n",
    "        stereo: Whether the spectrogram encodes stereo data\n",
    "        max_value: The max value of the original spectrogram. In practice doesn't matter.\n",
    "\n",
    "    Returns:\n",
    "        spectrogram: (channels, frequency, time)\n",
    "    \"\"\"\n",
    "    # Convert to RGB if single channel\n",
    "    if image.mode in (\"P\", \"L\"):\n",
    "        image = image.convert(\"RGB\")\n",
    "\n",
    "    # Flip Y\n",
    "    image = image.transpose(Image.Transpose.FLIP_TOP_BOTTOM)\n",
    "\n",
    "    # Munge channels into a numpy array of (channels, frequency, time)\n",
    "    data = np.array(image).transpose(2, 0, 1)\n",
    "    if stereo:\n",
    "        # Take the G and B channels as done in image_from_spectrogram\n",
    "        data = data[[1, 2], :, :]\n",
    "    else:\n",
    "        data = data[0:1, :, :]\n",
    "\n",
    "    # Convert to floats\n",
    "    data = data.astype(np.float32)\n",
    "\n",
    "    # Invert\n",
    "    data = 255 - data\n",
    "\n",
    "    # Rescale to 0-1\n",
    "    data = data / 255\n",
    "\n",
    "    # Reverse the power curve\n",
    "    data = np.power(data, 1 / power)\n",
    "\n",
    "    # Rescale to max value\n",
    "    data = data * max_value\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def audio_from_waveform(\n",
    "    samples: np.ndarray, sample_rate: int, normalize: bool = False\n",
    ") -> pydub.AudioSegment:\n",
    "    \"\"\"\n",
    "    Convert a numpy array of samples of a waveform to an audio segment.\n",
    "\n",
    "    Args:\n",
    "        samples: (channels, samples) array\n",
    "    \"\"\"\n",
    "    # Normalize volume to fit in int16\n",
    "    if normalize:\n",
    "        samples *= np.iinfo(np.int16).max / np.max(np.abs(samples))\n",
    "\n",
    "    # Transpose and convert to int16\n",
    "    samples = samples.transpose(1, 0)\n",
    "    samples = samples.astype(np.int16)\n",
    "\n",
    "    # Write to the bytes of a WAV file\n",
    "    wav_bytes = io.BytesIO()\n",
    "    wavfile.write(wav_bytes, sample_rate, samples)\n",
    "    wav_bytes.seek(0)\n",
    "\n",
    "    # Read into pydub\n",
    "    return pydub.AudioSegment.from_wav(wav_bytes)\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class SpectrogramParams:\n",
    "    \"\"\"\n",
    "    Parameters for the conversion from audio to spectrograms to images and back.\n",
    "\n",
    "    Includes helpers to convert to and from EXIF tags, allowing these parameters to be stored\n",
    "    within spectrogram images.\n",
    "\n",
    "    To understand what these parameters do and to customize them, read `spectrogram_converter.py`\n",
    "    and the linked torchaudio documentation.\n",
    "    \"\"\"\n",
    "\n",
    "    # Whether the audio is stereo or mono\n",
    "    stereo: bool = False\n",
    "\n",
    "    # FFT parameters\n",
    "    sample_rate: int = 44100\n",
    "    step_size_ms: int = 10\n",
    "    window_duration_ms: int = 100\n",
    "    padded_duration_ms: int = 400\n",
    "\n",
    "    # Mel scale parameters\n",
    "    num_frequencies: int = 512\n",
    "    # TODO(hayk): Set these to [20, 20000] for newer models\n",
    "    min_frequency: int = 0\n",
    "    max_frequency: int = 10000\n",
    "    mel_scale_norm: T.Optional[str] = None\n",
    "    mel_scale_type: str = \"htk\"\n",
    "    max_mel_iters: int = 200\n",
    "\n",
    "    # Griffin Lim parameters\n",
    "    num_griffin_lim_iters: int = 32\n",
    "\n",
    "    # Image parameterization\n",
    "    power_for_image: float = 0.25\n",
    "\n",
    "    class ExifTags(Enum):\n",
    "        \"\"\"\n",
    "        Custom EXIF tags for the spectrogram image.\n",
    "        \"\"\"\n",
    "\n",
    "        SAMPLE_RATE = 11000\n",
    "        STEREO = 11005\n",
    "        STEP_SIZE_MS = 11010\n",
    "        WINDOW_DURATION_MS = 11020\n",
    "        PADDED_DURATION_MS = 11030\n",
    "\n",
    "        NUM_FREQUENCIES = 11040\n",
    "        MIN_FREQUENCY = 11050\n",
    "        MAX_FREQUENCY = 11060\n",
    "\n",
    "        POWER_FOR_IMAGE = 11070\n",
    "        MAX_VALUE = 11080\n",
    "\n",
    "    @property\n",
    "    def n_fft(self) -> int:\n",
    "        \"\"\"\n",
    "        The number of samples in each STFT window, with padding.\n",
    "        \"\"\"\n",
    "        return int(self.padded_duration_ms / 1000.0 * self.sample_rate)\n",
    "\n",
    "    @property\n",
    "    def win_length(self) -> int:\n",
    "        \"\"\"\n",
    "        The number of samples in each STFT window.\n",
    "        \"\"\"\n",
    "        return int(self.window_duration_ms / 1000.0 * self.sample_rate)\n",
    "\n",
    "    @property\n",
    "    def hop_length(self) -> int:\n",
    "        \"\"\"\n",
    "        The number of samples between each STFT window.\n",
    "        \"\"\"\n",
    "        return int(self.step_size_ms / 1000.0 * self.sample_rate)\n",
    "\n",
    "    def to_exif(self) -> T.Dict[int, T.Any]:\n",
    "        \"\"\"\n",
    "        Return a dictionary of EXIF tags for the current values.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            self.ExifTags.SAMPLE_RATE.value: self.sample_rate,\n",
    "            self.ExifTags.STEREO.value: self.stereo,\n",
    "            self.ExifTags.STEP_SIZE_MS.value: self.step_size_ms,\n",
    "            self.ExifTags.WINDOW_DURATION_MS.value: self.window_duration_ms,\n",
    "            self.ExifTags.PADDED_DURATION_MS.value: self.padded_duration_ms,\n",
    "            self.ExifTags.NUM_FREQUENCIES.value: self.num_frequencies,\n",
    "            self.ExifTags.MIN_FREQUENCY.value: self.min_frequency,\n",
    "            self.ExifTags.MAX_FREQUENCY.value: self.max_frequency,\n",
    "            self.ExifTags.POWER_FOR_IMAGE.value: float(self.power_for_image),\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def from_exif(cls, exif: T.Mapping[int, T.Any]): \n",
    "        \"\"\"\n",
    "        Create a SpectrogramParams object from the EXIF tags of the given image.\n",
    "        \"\"\"\n",
    "        # TODO(hayk): Handle missing tags\n",
    "        return cls(\n",
    "            sample_rate=exif[cls.ExifTags.SAMPLE_RATE.value],\n",
    "            stereo=bool(exif[cls.ExifTags.STEREO.value]),\n",
    "            step_size_ms=exif[cls.ExifTags.STEP_SIZE_MS.value],\n",
    "            window_duration_ms=exif[cls.ExifTags.WINDOW_DURATION_MS.value],\n",
    "            padded_duration_ms=exif[cls.ExifTags.PADDED_DURATION_MS.value],\n",
    "            num_frequencies=exif[cls.ExifTags.NUM_FREQUENCIES.value],\n",
    "            min_frequency=exif[cls.ExifTags.MIN_FREQUENCY.value],\n",
    "            max_frequency=exif[cls.ExifTags.MAX_FREQUENCY.value],\n",
    "            power_for_image=exif[cls.ExifTags.POWER_FOR_IMAGE.value],\n",
    "        )\n",
    "\n",
    "\n",
    "params = SpectrogramParams()\n",
    "\n",
    "inverse_spectrogram_func = torchaudio.transforms.GriffinLim(\n",
    "            n_fft=params.n_fft,\n",
    "            n_iter=params.num_griffin_lim_iters,\n",
    "            win_length=params.win_length,\n",
    "            hop_length=params.hop_length,\n",
    "            window_fn=torch.hann_window,\n",
    "            power=1.0,\n",
    "            wkwargs=None,\n",
    "            momentum=0.99,\n",
    "            length=None,\n",
    "            rand_init=True,\n",
    "        ).to(device)\n",
    "\n",
    "\n",
    "inverse_mel_scaler = torchaudio.transforms.InverseMelScale(\n",
    "            n_stft=params.n_fft // 2 + 1,\n",
    "            n_mels=params.num_frequencies,\n",
    "            sample_rate=params.sample_rate,\n",
    "            f_min=params.min_frequency,\n",
    "            f_max=params.max_frequency,\n",
    "            norm=params.mel_scale_norm,\n",
    "            mel_scale=params.mel_scale_type,\n",
    "        ).to(device)\n",
    "\n",
    "def image_to_audio(img):\n",
    "    spectrogram = spectrogram_from_image(img)\n",
    "    amplitudes_mel = torch.from_numpy(spectrogram).to(device)\n",
    "    # Reconstruct the waveform\n",
    "    amplitudes_linear = inverse_mel_scaler(amplitudes_mel)\n",
    "    waveform = inverse_spectrogram_func(amplitudes_linear)\n",
    "\n",
    "    # Convert to audio segment\n",
    "    segment = audio_from_waveform(\n",
    "        samples=waveform.cpu().numpy(),\n",
    "        sample_rate=params.sample_rate,\n",
    "        # Normalize the waveform to the range [-1, 1]\n",
    "        normalize=True,\n",
    "    )\n",
    "    return segment\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|██████████| 7/7 [00:03<00:00,  2.12it/s]\n"
     ]
    }
   ],
   "source": [
    "pipeline = StableDiffusionPipeline.from_pretrained(\n",
    "            'riffusion-guzheng-v2')\n",
    "pipeline.to('cuda')\n",
    "generator = torch.Generator(device='cuda').manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:04<00:00,  4.86it/s]\n",
      "100%|██████████| 20/20 [00:04<00:00,  4.85it/s]\n",
      "100%|██████████| 20/20 [00:04<00:00,  4.82it/s]\n"
     ]
    }
   ],
   "source": [
    "prompts = ['lofi funk', 'happy pop', 'solo guzheng']\n",
    "for prompt in prompts:\n",
    "    images = pipeline('lofi funk', num_inference_steps=20, generator=generator)\n",
    "    audio = image_to_audio(images[0][0])\n",
    "    fname = f\"riffusion-guzheng-v2/{prompt.replace(' ', '_')}.wav\"\n",
    "    audio.export(fname)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GenerativeMusic-xr8A2ll8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
